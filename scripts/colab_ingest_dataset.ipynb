{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52221c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Colab setup\n",
    "!pip install langchain faiss-cpu sentence-transformers pytesseract pdfplumber opencv-python-headless\n",
    "\n",
    "import os\n",
    "import pytesseract\n",
    "import pdfplumber\n",
    "import cv2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = \"/content/dataset\"\n",
    "INDEX_DIR = \"/content/faiss_indexes\"\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def ocr_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 180, 255, cv2.THRESH_BINARY)\n",
    "    return pytesseract.image_to_string(thresh)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text.append(page.extract_text() or \"\")\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunks.append(\" \".join(words[i:i+chunk_size]))\n",
    "    return chunks\n",
    "\n",
    "texts = []\n",
    "sources = []\n",
    "\n",
    "for file in tqdm(list(Path(DATA_DIR).rglob(\"*\"))):\n",
    "    if file.suffix.lower() in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "        txt = ocr_image(str(file))\n",
    "        chunks = chunk_text(txt)\n",
    "        texts.extend(chunks)\n",
    "        sources.extend([str(file)] * len(chunks))\n",
    "    elif file.suffix.lower() == \".pdf\":\n",
    "        txt = extract_text_from_pdf(str(file))\n",
    "        chunks = chunk_text(txt)\n",
    "        texts.extend(chunks)\n",
    "        sources.extend([str(file)] * len(chunks))\n",
    "\n",
    "# Embed and store in FAISS\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(np.array(embeddings, dtype=\"float32\"))\n",
    "faiss.write_index(index, f\"{INDEX_DIR}/ncert_jee_neet.index\")\n",
    "\n",
    "print(f\"Indexed {len(texts)} chunks â†’ {INDEX_DIR}/ncert_jee_neet.index\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
